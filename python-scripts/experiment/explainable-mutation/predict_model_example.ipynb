{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b5049eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8cfac9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify these paths to your system's\n",
    "MODEL_METHOD_MUTATION_PATH = \"/home/chenghin/Desktop/repos/java-mutation-framework/models/code-generation/saved_models/checkpoint-best-score\"\n",
    "MODEL_COMMENT_MUTATION_PATH = \"/home/chenghin/Desktop/repos/java-mutation-framework/models/codet5_base_all_lr5_bs32_src64_trg64_pat5_e10/checkpoint-best-bleu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "435483af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Seq2Seq # Copy pasted from https://github.com/microsoft/CodeBERT/blob/master/UniXcoder/downstream-tasks/code-generation/model.py\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel, T5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "PRETRAINED_MODEL_NAME_UNIXCODER = \"microsoft/unixcoder-base\"\n",
    "PRETRAINED_MODEL_NAME_CODET5 = \"Salesforce/codet5-base-multi-sum\"\n",
    "\n",
    "class TokenizerModelPair:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "def build_model(pretrained_model_name):\n",
    "    if pretrained_model_name == PRETRAINED_MODEL_NAME_UNIXCODER:\n",
    "        # build model\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(pretrained_model_name)\n",
    "        config = RobertaConfig.from_pretrained(pretrained_model_name)\n",
    "        # importantÔºÅYou must set is_decoder to True for generation\n",
    "        config.is_decoder = True\n",
    "        encoder = RobertaModel.from_pretrained(pretrained_model_name,config=config)\n",
    "        model = Seq2Seq(encoder=encoder,decoder=encoder,config=config,\n",
    "                        beam_size=10,max_length=256,\n",
    "                        sos_id=tokenizer.convert_tokens_to_ids([\"<mask0>\"])[0],eos_id=tokenizer.sep_token_id)\n",
    "    else:\n",
    "        # build model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "        model = T5ForConditionalGeneration.from_pretrained(pretrained_model_name)\n",
    "        model.to(device)\n",
    "    return TokenizerModelPair(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5865f749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated!!!\n"
     ]
    }
   ],
   "source": [
    "unixcoder_model_and_tokenizer = build_model(PRETRAINED_MODEL_NAME_UNIXCODER)\n",
    "codet5_model_and_tokenizer = build_model(PRETRAINED_MODEL_NAME_CODET5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2d29dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def load_model(model, load_model_path='fine_tuned_models'):\n",
    "    model_to_load = model.module if hasattr(model, 'module') else model\n",
    "    load_model_path = os.path.join(load_model_path, 'pytorch_model.bin')\n",
    "    model.load_state_dict(torch.load(load_model_path, map_location='cpu'))\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "135ddfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model(unixcoder_model_and_tokenizer.model, MODEL_METHOD_MUTATION_PATH)\n",
    "load_model(codet5_model_and_tokenizer.model, MODEL_COMMENT_MUTATION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f36f8755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens\n",
    "\n",
    "def predict(model, tokenizer, code, gold):\n",
    "    input_ids = tokenizer(code, return_tensors=\"pt\").input_ids\n",
    "    generated_ids = model.generate(input_ids.to(device))\n",
    "    print(generated_ids)\n",
    "    comment = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return comment, getSmoothBLEU4(gold, comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "04259c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "chencherry = SmoothingFunction()\n",
    "def getSmoothBLEU4(gold, result):\n",
    "    BLEUscore = nltk.translate.bleu_score.sentence_bleu([gold.split()], result.split(), weights = [0.25,0.25,0.25,0.25], smoothing_function=chencherry.method2)\n",
    "    return round(BLEUscore,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a948115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = '''\n",
    "public RequestMethodsRequestCondition getMatchingCondition(ServerWebExchange exchange) {\n",
    "                if (CorsUtils.isPreFlightRequest(exchange.getRequest())) {\n",
    "                        return matchPreFlight(exchange.getRequest());\n",
    "                }\n",
    "                if (getMethods().isEmpty()) {\n",
    "                        if (RequestMethod.OPTIONS.name().equals(exchange.getRequest().getMethodValue())) {\n",
    "                                return null; // We handle OPTIONS transparently, so don't match if no explicit declarations\n",
    "                        }\n",
    "                        return this;\n",
    "                }\n",
    "                return matchRequestMethod(exchange.getRequest().getMethod());\n",
    "        }\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d9049fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = 'check if any of the http request methods match the given request and return an instance that contains the matching http request method only'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a26c7b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 653, 5128, 5763,  ...,    0,    0,    0],\n",
      "         [ 653, 5128, 5763,  ...,    0,    0,    0],\n",
      "         [1164, 5763, 1164,  ...,    0,    0,    0],\n",
      "         ...,\n",
      "         [ 653, 5128, 5763,  ...,    0,    0,    0],\n",
      "         [ 653, 5128, 5763,  ...,    0,    0,    0],\n",
      "         [1164, 5763, 1164,  ...,    0,    0,    0]]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43munixcoder_model_and_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munixcoder_model_and_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgold\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[91], line 7\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(model, tokenizer, code, gold)\u001b[0m\n\u001b[1;32m      5\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_ids)\n\u001b[0;32m----> 7\u001b[0m comment \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m comment, getSmoothBLEU4(gold, comment)\n",
      "File \u001b[0;32m~/anaconda3/envs/explainable-mutation/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3476\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3473\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3474\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3476\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3481\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/explainable-mutation/lib/python3.10/site-packages/transformers/tokenization_utils.py:931\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode\u001b[39m(\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    923\u001b[0m     token_ids: List[\u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    928\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_use_source_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_source_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 931\u001b[0m     filtered_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;66;03m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[39;00m\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;66;03m# we need to build string separately for added tokens and byte-level tokens\u001b[39;00m\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;66;03m# cf. https://github.com/huggingface/transformers/issues/1133\u001b[39;00m\n\u001b[1;32m    936\u001b[0m     sub_texts \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/explainable-mutation/lib/python3.10/site-packages/transformers/tokenization_utils.py:906\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    904\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[0;32m--> 906\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skip_special_tokens \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_ids:\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'list'"
     ]
    }
   ],
   "source": [
    "predict(unixcoder_model_and_tokenizer.model, unixcoder_model_and_tokenizer.tokenizer, code, gold) # test method mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ad3e6cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1, 32099,   430,  1281,   434,   326,  1062,   590,  2590,\n",
      "           845,   326,   864,   590,   471,   327,   392,   791,   716,  1914]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('if any of the http request methods match the given request and return an instance that contains',\n",
       " 0.0236)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(codet5_model_and_tokenizer.model, codet5_model_and_tokenizer.tokenizer, gold, code) # test comment mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292cc5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplainableMutator:\n",
    "    def __init__(self, comment_mutation_model, method_mutation_model):\n",
    "        self.comment_mutation_model = comment_mutation_model\n",
    "        self.method_mutation_model = method_mutation_model\n",
    "\n",
    "    def generate(self, comment, method_body):\n",
    "        mutated_comment = predict(self.comment_mutation_model, method_body, comment)[0]\n",
    "        mutated_method = predict(self.method_mutation_model, method_body, mutated_comment)[0]\n",
    "        return [mutated_comment, mutated_method]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae29429",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplainableMutationSocketServer(SocketServer):\n",
    "    def __init__(self, host, port, explainable_mutator):\n",
    "        super().__init__(host, port)\n",
    "        self.explainable_mutator = explainable_mutator\n",
    "\n",
    "    def func(self):\n",
    "        while True:\n",
    "            print(\"-\"*20)\n",
    "            comment = self.recvMsg()\n",
    "            method_body = self.recvMsg()\n",
    "            [mutated_comment, mutated_method] = self.explainable_mutator.generate(comment, method_body)\n",
    "            self.sendMsg(mutated_comment)\n",
    "            self.sendMsg(mutated_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7f7111",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"127.0.0.1\"\n",
    "PORT = 8080\n",
    "server = ExplainableMutationSocketServer(HOST, PORT, ExplainableMutator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "67addd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get matching condition.\n"
     ]
    }
   ],
   "source": [
    "    tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base-multi-sum')\n",
    "    model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base-multi-sum')\n",
    "\n",
    "    text = \"\"\"def svg_to_image(string, size=None):\n",
    "    if isinstance(string, unicode):\n",
    "        string = string.encode('utf-8')\n",
    "        renderer = QtSvg.QSvgRenderer(QtCore.QByteArray(string))\n",
    "    if not renderer.isValid():\n",
    "        raise ValueError('Invalid SVG data.')\n",
    "    if size is None:\n",
    "        size = renderer.defaultSize()\n",
    "        image = QtGui.QImage(size, QtGui.QImage.Format_ARGB32)\n",
    "        painter = QtGui.QPainter(image)\n",
    "        renderer.render(painter)\n",
    "    return image\"\"\"\n",
    "\n",
    "    input_ids = tokenizer(code, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    generated_ids = model.generate(input_ids, max_length=20)\n",
    "    print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0562c882",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
