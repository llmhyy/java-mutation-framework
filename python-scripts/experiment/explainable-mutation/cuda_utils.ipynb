{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6262cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, re, os, sys\n",
    "\n",
    "# GPU picking\n",
    "# http://stackoverflow.com/a/41638727/419116\n",
    "# Nvidia-smi GPU memory parsing.\n",
    "# Tested on nvidia-smi 370.23\n",
    "\n",
    "def run_command(cmd):\n",
    "    \"\"\"Run command, return output as string.\"\"\"\n",
    "    \n",
    "    output = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True).communicate()[0]\n",
    "    return output.decode(\"ascii\")\n",
    "\n",
    "def list_available_gpus():\n",
    "    \"\"\"Returns list of available GPU ids.\"\"\"\n",
    "    \n",
    "    output = run_command(\"nvidia-smi -L\")\n",
    "    # lines of the form GPU 0: TITAN X\n",
    "    gpu_regex = re.compile(r\"GPU (?P<gpu_id>\\d+):\")\n",
    "    result = []\n",
    "    for line in output.strip().split(\"\\n\"):\n",
    "        m = gpu_regex.match(line)\n",
    "        assert m, \"Couldnt parse \"+line\n",
    "        result.append(int(m.group(\"gpu_id\")))\n",
    "    return result\n",
    "\n",
    "def gpu_memory_map():\n",
    "    \"\"\"Returns map of GPU id to memory allocated on that GPU.\"\"\"\n",
    "\n",
    "    output = run_command(\"nvidia-smi\")\n",
    "    gpu_output = output[output.find(\"GPU Memory\"):]\n",
    "    # lines of the form\n",
    "    # |    0      8734    C   python                                       11705MiB |\n",
    "    memory_regex = re.compile(r\"[|]\\s+?(?P<gpu_id>\\d+)\\D+?(?P<pid>\\d+).+[ ](?P<gpu_memory>\\d+)MiB\")\n",
    "    rows = gpu_output.split(\"\\n\")\n",
    "    result = {gpu_id: 0 for gpu_id in list_available_gpus()}\n",
    "    for row in gpu_output.split(\"\\n\"):\n",
    "        m = memory_regex.search(row)\n",
    "        if not m:\n",
    "            continue\n",
    "        gpu_id = int(m.group(\"gpu_id\"))\n",
    "        gpu_memory = int(m.group(\"gpu_memory\"))\n",
    "        result[gpu_id] += gpu_memory\n",
    "    return result\n",
    "\n",
    "def pick_gpu_lowest_memory():\n",
    "    \"\"\"Returns GPU with the least allocated memory\"\"\"\n",
    "\n",
    "    memory_gpu_map = [(memory, gpu_id) for (gpu_id, memory) in gpu_memory_map().items()]\n",
    "    best_memory, best_gpu = sorted(memory_gpu_map)[0]\n",
    "    return best_gpu\n",
    "\n",
    "def setup_one_gpu():\n",
    "    assert not 'tensorflow' in sys.modules, \"GPU setup must happen before importing TensorFlow\"\n",
    "    gpu_id = pick_gpu_lowest_memory()\n",
    "    print(\"Picking GPU \"+str(gpu_id))\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "\n",
    "def setup_no_gpu():\n",
    "    if 'tensorflow' in sys.modules:\n",
    "        print(\"Warning, GPU setup must happen before importing TensorFlow\")\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = ''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:explainable-mutation] *",
   "language": "python",
   "name": "conda-env-explainable-mutation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
