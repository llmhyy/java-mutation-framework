{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8061a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if we want to debug the code. Set bpoints using pdb.set_trace()\n",
    "# %pdb on\n",
    "# import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac3659dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\bchenghi\\\\Desktop\\\\NUS\\\\FYP\\\\repos\\\\java-mutation-framework\\\\python-scripts\\\\experiment\\\\explainable-mutation', 'C:\\\\Users\\\\bchenghi\\\\anaconda3\\\\envs\\\\comment-mutation-exp\\\\python311.zip', 'C:\\\\Users\\\\bchenghi\\\\anaconda3\\\\envs\\\\comment-mutation-exp\\\\DLLs', 'C:\\\\Users\\\\bchenghi\\\\anaconda3\\\\envs\\\\comment-mutation-exp\\\\Lib', 'C:\\\\Users\\\\bchenghi\\\\anaconda3\\\\envs\\\\comment-mutation-exp', '', 'C:\\\\Users\\\\bchenghi\\\\anaconda3\\\\envs\\\\comment-mutation-exp\\\\Lib\\\\site-packages', 'C:\\\\Users\\\\bchenghi\\\\anaconda3\\\\envs\\\\comment-mutation-exp\\\\Lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\bchenghi\\\\anaconda3\\\\envs\\\\comment-mutation-exp\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\bchenghi\\\\anaconda3\\\\envs\\\\comment-mutation-exp\\\\Lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    }
   ],
   "source": [
    "# check if venv activated\n",
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e52e81cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cats', 0.8099379539489746),\n",
       " ('dog', 0.760945737361908),\n",
       " ('kitten', 0.7464984655380249),\n",
       " ('feline', 0.7326233983039856),\n",
       " ('beagle', 0.7150582671165466),\n",
       " ('puppy', 0.7075453996658325),\n",
       " ('pup', 0.6934291124343872),\n",
       " ('pet', 0.6891531348228455),\n",
       " ('felines', 0.6755931377410889),\n",
       " ('chihuahua', 0.6709762215614319)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "from os.path import exists\n",
    "\n",
    "MODEL_NAME = \"word2vec.model\"\n",
    "if 'model' not in globals():\n",
    "    print(\"model not yet initialized\")\n",
    "    if exists(MODEL_NAME):\n",
    "        print(\"Model found on system. Loading existing model...\")\n",
    "        model = KeyedVectors.load(MODEL_NAME)\n",
    "    else:\n",
    "        print(\"Downloading model from online...\")\n",
    "        model = api.load(\"word2vec-google-news-300\")  # download the model and return as object ready for use\n",
    "        model.save(MODEL_NAME) # save it locally\n",
    "model.most_similar(\"cat\") # check if working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ea4baae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP based\n",
    "import numpy as np\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation, strip_multiple_whitespaces, remove_stopwords\n",
    "from gensim.matutils import jaccard\n",
    "\n",
    "CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation, strip_multiple_whitespaces, remove_stopwords]\n",
    "\n",
    "def average_word_embedding(words):\n",
    "    word_embeddings = [model[word] for word in words if word in model]\n",
    "    if len(word_embeddings) == 0:\n",
    "        return None\n",
    "    return sum(word_embeddings) / len(word_embeddings)\n",
    "\n",
    "def get_cosine_sim(avg_embedding1, avg_embedding2):\n",
    "    return np.dot(avg_embedding1, avg_embedding2) / (np.linalg.norm(avg_embedding1) * np.linalg.norm(avg_embedding2))\n",
    "\n",
    "def get_jaccard(words1, words2):\n",
    "    return 1 - jaccard(set(words1), set(words2))\n",
    "\n",
    "def get_cosine(words1, words2):\n",
    "    avg_embedding1 = average_word_embedding(words1)\n",
    "    avg_embedding2 = average_word_embedding(words2)\n",
    "    return get_cosine_sim(avg_embedding1, avg_embedding2)\n",
    "\n",
    "def get_jac_and_cos(words1, words2):\n",
    "    return [get_jaccard(words1, words2), get_cosine(words1, words2)]\n",
    "\n",
    "def get_jac_and_cos_from_sentences(sentence0, sentence1, sentence2):\n",
    "    tokens0 = preprocess_string(sentence0, CUSTOM_FILTERS)\n",
    "    tokens1 = preprocess_string(sentence1, CUSTOM_FILTERS)\n",
    "    tokens2 = preprocess_string(sentence2, CUSTOM_FILTERS)\n",
    "    return [get_jac_and_cos(tokens0, tokens1), get_jac_and_cos(tokens0, tokens2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e1d70384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "SRC = \".src\"\n",
    "OUT = \".output\"\n",
    "GOLD = \".gold\"\n",
    "SRC_POS = 0\n",
    "OUT_POS = 1\n",
    "GOLD_POS = 2\n",
    "\n",
    "def get_lines_for_file(files):\n",
    "    result = []\n",
    "    for file in files:\n",
    "        result.append(file.readline())\n",
    "    return result\n",
    "\n",
    "def get_comments_for_lines(lines):\n",
    "    result = []\n",
    "    for line in lines:\n",
    "        result.append(line[line.index(\"\\t\") + 1:line.index(\"\\n\")])\n",
    "    return result\n",
    "\n",
    "def get_number_for_line(line):\n",
    "    return int(line[:line.index(\"\\t\")])\n",
    "\n",
    "def convert_to_csv(number, src_comment, gold_comment, out_comment, gold_jac_cos, out_jac_cos, err=\"\"):\n",
    "    arr =  [number, src_comment, gold_comment, out_comment, err]\n",
    "    arr.extend(np.array(gold_jac_cos).flatten())\n",
    "    arr.extend(np.array(out_jac_cos).flatten())\n",
    "    return arr\n",
    "\n",
    "def get_file_triplet(file_name_without_ext):\n",
    "    files = [None, None, None]\n",
    "    f_out = open(file_name_without_ext + OUT, 'r', encoding='utf-8')\n",
    "    f_src = open(file_name_without_ext + SRC, 'r', encoding='utf-8')\n",
    "    f_gold = open(file_name_without_ext + GOLD, 'r', encoding='utf-8')\n",
    "    files[SRC_POS] = f_src\n",
    "    files[OUT_POS] = f_out\n",
    "    files[GOLD_POS] = f_gold\n",
    "    return files\n",
    "\n",
    "# Count :\n",
    "    # Incomplete sentence\n",
    "    # did not have changes, when gold had\n",
    "    # Had changes, when gold did not\n",
    "    # given both had changes:\n",
    "        # Bigger difference in cosine + smaller difference in jaccard\n",
    "        # Larger diff in jaccard\n",
    "        # Smaller diff in cosine\n",
    "    # others\n",
    "class Analysis:\n",
    "    def __init__(self):\n",
    "        self.total = 0\n",
    "        self.incomplete = 0\n",
    "        self.both_no_change = 0\n",
    "        self.no_change = 0\n",
    "        self.better = 0\n",
    "        self.smaller_jac = 0\n",
    "        self.higher_cos = 0\n",
    "        self.others = 0\n",
    "        self.err = 0\n",
    "        # 12000, 5971, 1878, 3482, 218, 203, 77, 170, 0, 1\n",
    "    def __str__(self):\n",
    "        return \", \".join(list(map(lambda x: str(x), [self.total, self.incomplete, self.both_no_change, self.no_change, \n",
    "                                                     self.better, self.smaller_jac, self.higher_cos, \n",
    "                                                     self.others, self.err])))\n",
    "\n",
    "def analyse_row(row, analysis):\n",
    "    analysis.total += 1\n",
    "    err = row[4]\n",
    "    if (err != \"\"):\n",
    "        analysis.err += 1\n",
    "        return\n",
    "    out_sentence = row[3]\n",
    "    if (out_sentence[-1] != \".\"):\n",
    "        analysis.incomplete += 1\n",
    "        return\n",
    "    gold_sentence = row[2]\n",
    "    src_sentence = row[1]\n",
    "    gold_had_changes = src_sentence != gold_sentence\n",
    "    out_had_changes = src_sentence != out_sentence\n",
    "    if gold_had_changes and not out_had_changes:\n",
    "        analysis.no_change += 1\n",
    "        return\n",
    "    if not gold_had_changes and not out_had_changes:\n",
    "        analysis.both_no_change += 1\n",
    "        return\n",
    "    gold_jac = row[5]\n",
    "    gold_cos = row[6]\n",
    "    out_jac = row[7]\n",
    "    out_cos = row[8]\n",
    "    if (out_jac >= gold_jac and out_cos <= gold_cos):\n",
    "        analysis.better += 1\n",
    "        return\n",
    "    if (out_jac < gold_jac):\n",
    "        analysis.smaller_jac += 1\n",
    "        return\n",
    "    if out_cos > gold_cos:\n",
    "        analysis.higher_cos += 1\n",
    "        return\n",
    "    analysis.others += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f043f7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_average_word_embedding (__main__.TestFunctions.test_average_word_embedding) ... ok\n",
      "test_convert_to_csv (__main__.TestFunctions.test_convert_to_csv) ... C:\\Users\\bchenghi\\AppData\\Local\\Temp\\ipykernel_19856\\2120339883.py:66: DeprecationWarning: Please use assertEqual instead.\n",
      "  self.assertEquals(row, [10,\"src_comment\",\"gold_comment\",\"out_comment\",\"\",1.0,0.5,0,2])\n",
      "ok\n",
      "test_get_comments_for_lines (__main__.TestFunctions.test_get_comments_for_lines) ... ok\n",
      "test_get_jac_and_cos_from_sentences (__main__.TestFunctions.test_get_jac_and_cos_from_sentences) ... ok\n",
      "test_get_jaccard (__main__.TestFunctions.test_get_jaccard) ... ok\n",
      "test_get_lines_for_file (__main__.TestFunctions.test_get_lines_for_file) ... ok\n",
      "test_get_number_for_line (__main__.TestFunctions.test_get_number_for_line) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 7 tests in 0.061s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x157417bd890>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tests\n",
    "import unittest\n",
    "from numpy.testing import assert_array_equal\n",
    "\n",
    "TEST_FILES_PATH = \"./test-files\"\n",
    "\n",
    "class TestFunctions(unittest.TestCase):\n",
    "    files_opened = []\n",
    "    \n",
    "    def setUp(self):\n",
    "        self.files_opened = []\n",
    "\n",
    "    def tearDown(self):\n",
    "        for file in self.files_opened:\n",
    "            file.close()\n",
    "\n",
    "# ======================== Integration tests (could be shifted else where) =====================\n",
    "    def test_get_lines_for_file(self):\n",
    "        file_name = \"test_e0\"\n",
    "        files = get_file_triplet(TEST_FILES_PATH + \"/\" + file_name)\n",
    "        self.files_opened.extend(files)\n",
    "        lines = get_lines_for_file(files)\n",
    "        expected = ['989\\tThis method sends and handles a connection factory, object and response handler to create a ResponseStream. If the active flag is false, an IOException is thrown. If the ErrorHandler throws a retryable error, the operation is retried, else an IOException is thrown. Finally, the response is handled and the stream is ended.\\n', '989\\tThis method sends and handles a connection factory, object and response handler to create a ResponseStream. If the active flag is false, an IOException is thrown. If the ErrorHandler throws a retryable error, the operation is retried, else an IOException is thrown. Finally, the response is handled and the stream is ended\\n', '989\\tThis method sends and handles a connection factory, object and response handler to create a ResponseStream. If the active flag is false, an HazelcastInstanceNotActiveException is thrown. If the ErrorHandler throws a retryable error, the operation is retried, else an IOException is thrown. Finally, the response is handled and the stream is ended.\\n']\n",
    "        self.assertEquals(lines, expected)\n",
    "\n",
    "    def test_get_jac_and_cos_from_sentences(self): \n",
    "        sentence1 = \"This is the first sentence.\"\n",
    "        sentence2 = \"This is the second sentence.\"\n",
    "        sentence3 = \"Hello world.\"\n",
    "        jac_cos_res = get_jac_and_cos_from_sentences(sentence1, sentence2, sentence3)\n",
    "        expected_jac_cos = [[0.5, 0.90152967], [0.0, 0.03786046]]\n",
    "        np.testing.assert_almost_equal(jac_cos_res, expected_jac_cos, 5)\n",
    "\n",
    "# ======================== Unit tests =====================\n",
    "    def test_get_comments_for_lines(self):\n",
    "        numberStr = \"20\"\n",
    "        lines = [numberStr + \"\\tcomment0\\n\", numberStr + \"\\tcomment1\\n\", numberStr + \"\\tcomment2\\n\"]\n",
    "        comments = get_comments_for_lines(lines)\n",
    "        self.assertEquals(comments, ['comment0', 'comment1', 'comment2'])\n",
    "    \n",
    "    def test_get_number_for_line(self):\n",
    "        number = 10\n",
    "        numberStr = str(number)\n",
    "        line = numberStr + \"\\tcomment0\\n\"\n",
    "        actual_num = get_number_for_line(line)\n",
    "        self.assertEquals(actual_num, number)\n",
    "    \n",
    "    def test_average_word_embedding(self):\n",
    "        sentence1 = \"This is the first sentence.\".split()\n",
    "        sentence2 = \"This is the second sentence.\".split()\n",
    "        sentence3 = \"Hello world.\".split()\n",
    "        avg_embedding1 = average_word_embedding(sentence1)\n",
    "        avg_embedding2 = average_word_embedding(sentence2)\n",
    "        avg_embedding3 = average_word_embedding(sentence3)\n",
    "        if avg_embedding1 is not None and avg_embedding2 is not None and avg_embedding3 is not None:\n",
    "            cosine_sim = get_cosine_sim(avg_embedding1, avg_embedding2)\n",
    "            cosine_sim1 = get_cosine_sim(avg_embedding1, avg_embedding3)\n",
    "            self.assertTrue(cosine_sim > cosine_sim1)\n",
    "        else:\n",
    "            self.fail(\"One or both of the sentences has no word embeddings.\")\n",
    "\n",
    "    def test_convert_to_csv(self):\n",
    "        gold_jac_cos = [1, 0.5]\n",
    "        out_jac_cos = [0, 2]\n",
    "        row = convert_to_csv(10, \"src_comment\", \"gold_comment\", \"out_comment\", gold_jac_cos, out_jac_cos)\n",
    "        self.assertEquals(row, [10,\"src_comment\",\"gold_comment\",\"out_comment\",\"\",1.0,0.5,0,2])\n",
    "    \n",
    "    def test_get_jaccard(self):\n",
    "        words1= [\"word\", \"word1\"]\n",
    "        words2 = [\"word\", \"word1\"]\n",
    "        actual = get_jaccard(words1, words2)\n",
    "        self.assertEquals(actual, 1)\n",
    "        \n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b8ec689e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000, 973, 359, 560, 63, 30, 14, 0, 1\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import json\n",
    "\n",
    "PATH = \"C:\\\\Users\\\\bchenghi\\\\Desktop\\\\NUS\\\\FYP\\\\explainable-data\\\\experiment\\\\comment_mutate\\\\prediction\"\n",
    "PATH_TO_TEST_JSONL = \"C:\\\\Users\\\\bchenghi\\\\Desktop\\\\NUS\\\\FYP\\\\explainable-data\\\\data\\\\comment_mutate\\\\test.jsonl\"\n",
    "CUSTOM_DELIM = \";\"\n",
    "file_names = [\"test_best-bleu\"]\n",
    "ls_of_tokens = []\n",
    "\n",
    "# Open the JSONL file for reading\n",
    "with open(PATH_TO_TEST_JSONL, 'r', encoding='utf-8') as jsonl_file:\n",
    "\n",
    "    # Read each line (which contains a single JSON object)\n",
    "    for line in jsonl_file:\n",
    "\n",
    "        # Convert the JSON string to a Python object\n",
    "        json_object = json.loads(line)\n",
    "        tokens = json_object[\"code_tokens\"]\n",
    "        ls_of_tokens.append(tokens)\n",
    "        \n",
    "def get_line_count(file_path):\n",
    "    f = open(file_path + SRC, \"r\", encoding='utf-8')\n",
    "    line_count = sum(1 for x in f)\n",
    "    f.close()\n",
    "    return line_count\n",
    "\n",
    "for i in range(0, 10):\n",
    "    file_name = \"test_e\" + str(i)\n",
    "    file_names.append(file_name)\n",
    "\n",
    "final_analysis = Analysis()\n",
    "\n",
    "for i, file_name in enumerate(file_names):\n",
    "    csv_file_name = \"jac_cos_result_\" + str(i) + \".csv\"\n",
    "    if (os.path.exists(csv_file_name)):\n",
    "        os.remove(csv_file_name)\n",
    "    with open(csv_file_name, \"a\", newline='') as csv_file:\n",
    "        csv_file.write(\"sep=\" + CUSTOM_DELIM + os.linesep)\n",
    "        writer = csv.writer(csv_file, delimiter=CUSTOM_DELIM)\n",
    "        final_path = PATH + \"\\\\\" + file_name\n",
    "        files = get_file_triplet(final_path)\n",
    "        line_cnt = 0\n",
    "        for _ in range(get_line_count(final_path)):\n",
    "            try:\n",
    "                lines = get_lines_for_file(files)\n",
    "                number = get_number_for_line(lines[0])\n",
    "                line_cnt += 1\n",
    "                comments = get_comments_for_lines(lines)\n",
    "                at_least_one = False\n",
    "                for tokens in ls_of_tokens:\n",
    "                    found = True\n",
    "                    for token in tokens:\n",
    "                        if token not in comments[0]:\n",
    "                            found = False\n",
    "                            break\n",
    "                    if found:\n",
    "                        at_least_one = True\n",
    "                        break\n",
    "                if not at_least_one:\n",
    "                    continue\n",
    "                gold_jac_and_cos = get_jac_and_cos(comments[SRC_POS], comments[GOLD_POS])\n",
    "                out_jac_and_cos = get_jac_and_cos(comments[SRC_POS], comments[OUT_POS])\n",
    "                csv_row = convert_to_csv(number, comments[SRC_POS], comments[GOLD_POS], comments[OUT_POS], gold_jac_and_cos, out_jac_and_cos)\n",
    "                analyse_row(csv_row, final_analysis)\n",
    "                writer.writerow(csv_row)\n",
    "            except Exception as e:\n",
    "                csv_row = convert_to_csv(number, \"\", \"\", \"\", [0,0], [0,0], str(e))\n",
    "                analyse_row(csv_row, final_analysis)\n",
    "                writer.writerow(csv_row)\n",
    "        for file in files:\n",
    "            file.close()\n",
    "print(final_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e42abb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
